{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DWT features Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pywt\n",
    "from scipy.stats import skew, kurtosis\n",
    "from skimage import color, io\n",
    "from skimage.measure import shannon_entropy\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Path to your dataset folder\n",
    "dataset_folder = r\"D:\\new dataset\\KTH_TIPS\"\n",
    "\n",
    "# Function to preprocess the image\n",
    "def preprocess_image(image_path, target_size=(128, 128)):\n",
    "    # Load the image\n",
    "    img = io.imread(image_path)\n",
    "    if img.shape[-1] == 4:  # RGBA to RGB\n",
    "        img = img[:, :, :3]\n",
    "    resized_img = cv2.resize(img, target_size)  # Resize to target size\n",
    "    normalized_img = resized_img / 255.0  # Normalize to [0, 1]\n",
    "    return normalized_img\n",
    "\n",
    "# Function to calculate features from wavelet sub-bands\n",
    "def calculate_wavelet_features(image):\n",
    "    # Perform 2D Discrete Wavelet Transform (DWT)\n",
    "    coeffs_dwt = pywt.dwt2(image, 'haar')\n",
    "    LL, (LH, HL, HH) = coeffs_dwt\n",
    "\n",
    "    # Sub-band names and values\n",
    "    subbands = {\"LL\": LL, \"LH\": LH, \"HL\": HL, \"HH\": HH}\n",
    "    features = {}\n",
    "\n",
    "    # Extract statistical features for each sub-band\n",
    "    for band, subband in subbands.items():\n",
    "        subband_flat = subband.flatten()  # Flatten for statistical calculations\n",
    "        features.update({\n",
    "            f\"{band}_Mean\": np.mean(subband),\n",
    "            f\"{band}_StdDev\": np.std(subband),\n",
    "            f\"{band}_Variance\": np.var(subband),\n",
    "            f\"{band}_Energy\": np.sum(np.square(subband)),\n",
    "            f\"{band}_Entropy\": shannon_entropy(subband),\n",
    "            f\"{band}_Skewness\": skew(subband_flat),\n",
    "            f\"{band}_Kurtosis\": kurtosis(subband_flat),\n",
    "            f\"{band}_Min\": np.min(subband),\n",
    "            f\"{band}_Max\": np.max(subband),\n",
    "            f\"{band}_Range\": np.ptp(subband),  # Peak-to-peak (Max-Min)\n",
    "        })\n",
    "    return features\n",
    "\n",
    "# Main feature extraction pipeline\n",
    "feature_data = []\n",
    "\n",
    "for class_name in os.listdir(dataset_folder):\n",
    "    class_path = os.path.join(dataset_folder, class_name)\n",
    "    if os.path.isdir(class_path):\n",
    "        files = [f for f in os.listdir(class_path) if f.endswith(('.jpg', '.png', '.jpeg', '.jfif'))]\n",
    "        with tqdm(total=len(files), desc=f\"Processing {class_name}\", unit=\"file\") as pbar:\n",
    "            for file_name in files:\n",
    "                image_path = os.path.join(class_path, file_name)\n",
    "\n",
    "                # Preprocess the image\n",
    "                grayscale_image = preprocess_image(image_path)\n",
    "\n",
    "                # Extract wavelet features\n",
    "                wavelet_features = calculate_wavelet_features(grayscale_image)\n",
    "\n",
    "                # Combine all features\n",
    "                combined_features = {\n",
    "                    \"Class\": class_name,\n",
    "                    \"Image Name\": file_name,\n",
    "                    **wavelet_features,\n",
    "                }\n",
    "\n",
    "                # Append features to the dataset\n",
    "                feature_data.append(combined_features)\n",
    "                pbar.update(1)\n",
    "\n",
    "# Create a DataFrame and save it as a CSV file\n",
    "df = pd.DataFrame(feature_data)\n",
    "output_csv_path = r\"D:\\new dataset\\new dataset\\DWT\\DWT_wavelet_features.csv\"\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "print(\"Feature extraction completed. Data saved to:\", output_csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GLCM features Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "from scipy.stats import skew, kurtosis\n",
    "from skimage.measure import shannon_entropy\n",
    "\n",
    "# Path to the dataset folders\n",
    "dataset_path = r\"D:\\new dataset\\KTH_TIPS\"\n",
    "categories = [\"aluminium_foil\", \"brown_bread\", \"corduroy\", \"cotton\", \"cracker\", \"orange_peel\",\"linen\",\"sandpaper\",\"sponge\",\"styrofoam\"]  # Subfolders for fresh and rotten fruits\n",
    "\n",
    "\n",
    "# Function to extract features for a single image\n",
    "def extract_features(image_path, patch_size=7):  # Increased patch size for better performance\n",
    "    # Load and preprocess the image\n",
    "    image = cv2.imread(image_path)\n",
    "    # For only 3 channels (RGB)\n",
    "    if image.shape[-1] == 4:\n",
    "        image = image[:, :, :3]\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    resized_image = cv2.resize(gray_image, (128, 128))  # Reduced size for faster computation\n",
    "\n",
    "    # Initialize feature maps\n",
    "    contrast_map = np.zeros_like(resized_image, dtype=float)\n",
    "    homogeneity_map = np.zeros_like(resized_image, dtype=float)\n",
    "    energy_map = np.zeros_like(resized_image, dtype=float)\n",
    "    correlation_map = np.zeros_like(resized_image, dtype=float)\n",
    "    entropy_map = np.zeros_like(resized_image, dtype=float)\n",
    "\n",
    "    # Calculate local GLCM features using a sliding window\n",
    "    for i in range(0, resized_image.shape[0] - patch_size, patch_size):\n",
    "        for j in range(0, resized_image.shape[1] - patch_size, patch_size):\n",
    "            patch = resized_image[i:i + patch_size, j:j + patch_size]\n",
    "            patch_glcm = graycomatrix(patch, distances=[1], angles=[0], levels=256, symmetric=True, normed=True)\n",
    "            contrast_map[i:i + patch_size, j:j + patch_size] = graycoprops(patch_glcm, 'contrast')[0, 0]\n",
    "            homogeneity_map[i:i + patch_size, j:i + patch_size] = graycoprops(patch_glcm, 'homogeneity')[0, 0]\n",
    "            energy_map[i:i + patch_size, j:i + patch_size] = graycoprops(patch_glcm, 'energy')[0, 0]\n",
    "            correlation_map[i:i + patch_size, j:i + patch_size] = graycoprops(patch_glcm, 'correlation')[0, 0]\n",
    "            entropy_map[i:i + patch_size, j:i + patch_size] = -np.sum(patch_glcm * np.log2(patch_glcm + (patch_glcm == 0)))\n",
    "\n",
    "    # Compute global averages of the feature maps\n",
    "    features = {\n",
    "        \"contrast\": np.mean(contrast_map),\n",
    "        \"homogeneity\": np.mean(homogeneity_map),\n",
    "        \"energy\": np.mean(energy_map),\n",
    "        \"correlation\": np.mean(correlation_map),\n",
    "        \"entropy\": np.mean(entropy_map),\n",
    "        \"mean\": np.mean(resized_image),\n",
    "        \"std_deviation\": np.std(resized_image),\n",
    "        \"variance\": np.var(resized_image),\n",
    "        \"skewness\": skew(resized_image.flatten()),\n",
    "        \"kurtosis\": kurtosis(resized_image.flatten()),\n",
    "        \"min\": np.min(resized_image),\n",
    "        \"max\": np.max(resized_image),\n",
    "        \"range\": np.ptp(resized_image)\n",
    "    }\n",
    "    return features\n",
    "\n",
    "# Initialize a list to store data\n",
    "data = []\n",
    "\n",
    "# Progress tracking variables\n",
    "total_images = sum(len(files) for _, _, files in os.walk(dataset_path) if files)\n",
    "processed_images = 0\n",
    "\n",
    "# Iterate through the dataset\n",
    "for category in categories:\n",
    "    category_path = os.path.join(dataset_path, category)\n",
    "    label = category  # Label is the category name\n",
    "\n",
    "    for filename in os.listdir(category_path):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):  # Add other extensions if needed\n",
    "            image_path = os.path.join(category_path, filename)\n",
    "            features = extract_features(image_path)\n",
    "            \n",
    "            # Include filename and category in the features\n",
    "            features[\"filename\"] = filename  # Add the filename\n",
    "            features[\"category\"] = label  # Add the label (category)\n",
    "\n",
    "            data.append(features)\n",
    "\n",
    "            # Update progress\n",
    "            processed_images += 1\n",
    "            progress = (processed_images / total_images) * 100\n",
    "            print(f\"Progress: {progress:.2f}% images processed\", end=\"\\r\")\n",
    "\n",
    "# Convert the data to a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the first 5 rows of the DataFrame\n",
    "print(\"\\nFirst 5 rows of the DataFrame:\")\n",
    "print(df.head())\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_csv_path = r\"D:\\new dataset\\new dataset\\GLCM\\glcm_features.csv\"  # Replace with your desired CSV file path\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"\\nFeature extraction completed. Data saved to {output_csv_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SWT Features Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pywt\n",
    "from skimage import io, color\n",
    "from skimage.measure import shannon_entropy\n",
    "from tqdm import tqdm\n",
    "import cv2  # For resizing\n",
    "\n",
    "# Path to your dataset folder (update this path)\n",
    "dataset_folder =r\"D:\\new dataset\\KTH_TIPS\"\n",
    "\n",
    "# Function to preprocess the image\n",
    "def preprocess_image(image_path, target_size=(128, 128)):\n",
    "    # Load the image\n",
    "    img = io.imread(image_path)\n",
    "    \n",
    "    # If the image has an alpha channel, remove it (convert RGBA to RGB)\n",
    "    if img.ndim == 3 and img.shape[-1] == 4:\n",
    "        img = img[:, :, :3]\n",
    "    \n",
    "    # Convert RGB to grayscale if the image is not already grayscale\n",
    "    if img.ndim == 3:  # Check if the image has 3 channels\n",
    "        grayscale_img = color.rgb2gray(img)\n",
    "    else:\n",
    "        grayscale_img = img  # The image is already grayscale\n",
    "    \n",
    "    # Resize the grayscale image to the target size\n",
    "    resized_img = cv2.resize(grayscale_img, target_size)\n",
    "    \n",
    "    # Normalize pixel values to the range [0, 1]\n",
    "    normalized_img = resized_img / 255.0\n",
    "    \n",
    "    return normalized_img\n",
    "\n",
    "# Function to calculate features for each sub-band\n",
    "def calculate_features(sub_band):\n",
    "    features = {}\n",
    "    # Calculate mean\n",
    "    features['Mean'] = np.mean(sub_band)\n",
    "    \n",
    "    # Calculate standard deviation\n",
    "    features['Std_Dev'] = np.std(sub_band)\n",
    "    \n",
    "    # Calculate variance\n",
    "    features['Variance'] = np.var(sub_band)\n",
    "    \n",
    "    # Calculate energy (sum of squared values)\n",
    "    features['Energy'] = np.sum(np.square(sub_band))\n",
    "    \n",
    "    # Calculate entropy using Shannon entropy\n",
    "    features['Entropy'] = shannon_entropy(sub_band)\n",
    "    \n",
    "    # Calculate contrast (standard deviation as a simple measure)\n",
    "    features['Contrast'] = np.std(sub_band)\n",
    "    \n",
    "    # Calculate homogeneity\n",
    "    features['Homogeneity'] = 1 / (1 + features['Variance'])\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Function to calculate correlation between sub-bands\n",
    "def calculate_correlation(sub_band1, sub_band2):\n",
    "    return np.corrcoef(sub_band1.flatten(), sub_band2.flatten())[0, 1]\n",
    "\n",
    "# Initialize a list to store feature data for the DataFrame\n",
    "feature_data = []\n",
    "\n",
    "# Loop through each class folder (e.g., apple, peach, etc.)\n",
    "for class_name in os.listdir(dataset_folder):\n",
    "    class_path = os.path.join(dataset_folder, class_name)\n",
    "    \n",
    "    # Process only directories\n",
    "    if os.path.isdir(class_path):\n",
    "        total_files = len([f for f in os.listdir(class_path) if f.endswith(('.jpg', '.png', '.jpeg', '.jfif'))])\n",
    "        \n",
    "        with tqdm(total=total_files, desc=f\"Processing {class_name} images\", unit=\"file\") as pbar:\n",
    "            for file_name in os.listdir(class_path):\n",
    "                if file_name.endswith(('.jpg', '.png', '.jpeg', '.jfif')):\n",
    "                    image_path = os.path.join(class_path, file_name)\n",
    "                    \n",
    "                    # Preprocess the image\n",
    "                    preprocessed_image = preprocess_image(image_path)\n",
    "                    \n",
    "                    # Perform 2D SWT (Stationary Wavelet Transform)\n",
    "                    coeffs = pywt.swt2(preprocessed_image, 'haar', level=1)  # Single-level SWT\n",
    "                    LL, (LH, HL, HH) = coeffs[0]  # Extract subbands from level 1\n",
    "                    \n",
    "                    # Calculate features for each sub-band\n",
    "                    features_LL = calculate_features(LL)\n",
    "                    features_LH = calculate_features(LH)\n",
    "                    features_HL = calculate_features(HL)\n",
    "                    features_HH = calculate_features(HH)\n",
    "                    \n",
    "                    # Calculate correlations between sub-bands\n",
    "                    correlation_LL_LH = calculate_correlation(LL, LH)\n",
    "                    correlation_LL_HL = calculate_correlation(LL, HL)\n",
    "                    correlation_LL_HH = calculate_correlation(LL, HH)\n",
    "                    correlation_LH_HL = calculate_correlation(LH, HL)\n",
    "                    correlation_LH_HH = calculate_correlation(LH, HH)\n",
    "                    correlation_HL_HH = calculate_correlation(HL, HH)\n",
    "                    \n",
    "                    # Combine features and add class label\n",
    "                    feature_row = {\n",
    "                        'Class': class_name,\n",
    "                        'Image Name': file_name,\n",
    "                        # Features for LL sub-band\n",
    "                        'LL_Mean': features_LL['Mean'], 'LL_Std_Dev': features_LL['Std_Dev'], \n",
    "                        'LL_Variance': features_LL['Variance'], 'LL_Energy': features_LL['Energy'],\n",
    "                        'LL_Entropy': features_LL['Entropy'], 'LL_Contrast': features_LL['Contrast'],\n",
    "                        'LL_Homogeneity': features_LL['Homogeneity'],\n",
    "                        \n",
    "                        # Features for LH sub-band\n",
    "                        'LH_Mean': features_LH['Mean'], 'LH_Std_Dev': features_LH['Std_Dev'], \n",
    "                        'LH_Variance': features_LH['Variance'], 'LH_Energy': features_LH['Energy'],\n",
    "                        'LH_Entropy': features_LH['Entropy'], 'LH_Contrast': features_LH['Contrast'],\n",
    "                        'LH_Homogeneity': features_LH['Homogeneity'],\n",
    "                        \n",
    "                        # Features for HL sub-band\n",
    "                        'HL_Mean': features_HL['Mean'], 'HL_Std_Dev': features_HL['Std_Dev'], \n",
    "                        'HL_Variance': features_HL['Variance'], 'HL_Energy': features_HL['Energy'],\n",
    "                        'HL_Entropy': features_HL['Entropy'], 'HL_Contrast': features_HL['Contrast'],\n",
    "                        'HL_Homogeneity': features_HL['Homogeneity'],\n",
    "                        \n",
    "                        # Features for HH sub-band\n",
    "                        'HH_Mean': features_HH['Mean'], 'HH_Std_Dev': features_HH['Std_Dev'], \n",
    "                        'HH_Variance': features_HH['Variance'], 'HH_Energy': features_HH['Energy'],\n",
    "                        'HH_Entropy': features_HH['Entropy'], 'HH_Contrast': features_HH['Contrast'],\n",
    "                        'HH_Homogeneity': features_HH['Homogeneity'],\n",
    "                        \n",
    "                        # Correlations between sub-bands\n",
    "                        'LL_LH_Correlation': correlation_LL_LH, 'LL_HL_Correlation': correlation_LL_HL,\n",
    "                        'LL_HH_Correlation': correlation_LL_HH, 'LH_HL_Correlation': correlation_LH_HL,\n",
    "                        'LH_HH_Correlation': correlation_LH_HH, 'HL_HH_Correlation': correlation_HL_HH\n",
    "                    }\n",
    "                    feature_data.append(feature_row)\n",
    "                    pbar.update(1)  # Update progress bar\n",
    "\n",
    "# Create a DataFrame from the feature data\n",
    "df = pd.DataFrame(feature_data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_csv_path = r\"D:\\new dataset\\new dataset\\swt\\sWT_features_with_labels.csv\"\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(\"Feature extraction completed. Data saved to:\", output_csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SWT+GLCM FEATURES EXTRACTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pywt\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "from skimage.measure import shannon_entropy\n",
    "\n",
    "# Path to the dataset folders\n",
    "dataset_path = r\"D:\\new dataset\\KTH_TIPS\"\n",
    "categories = [\"aluminium_foil\", \"brown_bread\", \"corduroy\", \"cotton\", \"cracker\", \"orange_peel\",\"linen\",\"sandpaper\",\"sponge\",\"styrofoam\"] \n",
    "\n",
    "# Function to preprocess the image\n",
    "def preprocess_image(image_path, target_size=(128, 128)):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image.shape[-1] == 4:  # If RGBA, convert to RGB\n",
    "        image = image[:, :, :3]\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
    "    resized_image = cv2.resize(gray_image, target_size)  # Resize to uniform size\n",
    "    return resized_image\n",
    "\n",
    "# Function to extract SWT and GLCM features\n",
    "def extract_swt_glcm_features(image_path):\n",
    "    # Preprocess the image\n",
    "    preprocessed_image = preprocess_image(image_path)\n",
    "\n",
    "    # Apply Stationary Wavelet Transform (SWT)\n",
    "    coeffs_swt = pywt.swt2(preprocessed_image, wavelet='haar', level=1, norm=True)\n",
    "    LL, (LH, HL, HH) = coeffs_swt[0]  # Extract the first-level subbands\n",
    "\n",
    "    # Extract GLCM features from the LL subband\n",
    "    glcm = graycomatrix(LL.astype(np.uint8), distances=[1], angles=[0, np.pi/4, np.pi/2, 3*np.pi/4], levels=256, symmetric=True, normed=True)\n",
    "\n",
    "    # Calculate GLCM properties\n",
    "    glcm_features = {\n",
    "        \"contrast\": np.mean(graycoprops(glcm, 'contrast')),\n",
    "        \"homogeneity\": np.mean(graycoprops(glcm, 'homogeneity')),\n",
    "        \"energy\": np.mean(graycoprops(glcm, 'energy')),\n",
    "        \"correlation\": np.mean(graycoprops(glcm, 'correlation')),\n",
    "        \"dissimilarity\": np.mean(graycoprops(glcm, 'dissimilarity')),\n",
    "        \"ASM\": np.mean(graycoprops(glcm, 'ASM')),  # Angular Second Moment\n",
    "        \"entropy\": shannon_entropy(LL)  # Entropy of the LL subband\n",
    "    }\n",
    "    return glcm_features\n",
    "\n",
    "# Initialize a list to store the extracted features\n",
    "data = []\n",
    "\n",
    "# Progress tracking\n",
    "total_images = sum(len(files) for _, _, files in os.walk(dataset_path) if files)\n",
    "processed_images = 0\n",
    "\n",
    "# Iterate through the dataset\n",
    "for category in categories:\n",
    "    category_path = os.path.join(dataset_path, category)\n",
    "    label = category  # Use the folder name as the label\n",
    "\n",
    "    for filename in os.listdir(category_path):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "            image_path = os.path.join(category_path, filename)\n",
    "            features = extract_swt_glcm_features(image_path)\n",
    "            features[\"filename\"] = filename\n",
    "            features[\"category\"] = label\n",
    "            data.append(features)\n",
    "\n",
    "            # Update progress\n",
    "            processed_images += 1\n",
    "            progress = (processed_images / total_images) * 100\n",
    "            print(f\"Progress: {progress:.2f}% images processed\", end=\"\\r\")\n",
    "\n",
    "# Convert the extracted features into a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_csv_path = r\"D:\\new dataset\\new dataset\\swt+glcm\\swt_glcm_features.csv\"\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"\\nFeature extraction completed. Data saved to {output_csv_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DWT+GLCM Features Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pywt\n",
    "from scipy.stats import skew, kurtosis\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "from skimage.measure import shannon_entropy\n",
    "\n",
    "# Path to the dataset\n",
    "dataset_path = r\"D:\\new dataset\\KTH_TIPS\"\n",
    "categories = [\"aluminium_foil\", \"brown_bread\", \"corduroy\", \"cotton\", \"cracker\", \"orange_peel\",\"linen\",\"sandpaper\",\"sponge\",\"styrofoam\"] \n",
    "\n",
    "# Function to preprocess the image\n",
    "def preprocess_image(image_path, target_size=(128, 128)):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image.shape[-1] == 4:  # If RGBA, convert to RGB\n",
    "        image = image[:, :, :3]\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
    "    resized_image = cv2.resize(gray_image, target_size)  # Resize to uniform size\n",
    "    return resized_image\n",
    "\n",
    "# Function to apply DWT and extract features\n",
    "def extract_dwt_glcm_features(image_path):\n",
    "    # Preprocess the image\n",
    "    preprocessed_image = preprocess_image(image_path)\n",
    "\n",
    "    # Apply Discrete Wavelet Transform (DWT)\n",
    "    coeffs_dwt = pywt.dwt2(preprocessed_image, wavelet='haar')\n",
    "    LL, (LH, HL, HH) = coeffs_dwt  # Extract subbands\n",
    "\n",
    "    # Extract GLCM features from the LL subband\n",
    "    glcm = graycomatrix(LL.astype(np.uint8), distances=[1], angles=[0, np.pi/4, np.pi/2, 3*np.pi/4], levels=512, symmetric=True, normed=True)\n",
    "\n",
    "    # Calculate GLCM properties\n",
    "    glcm_features = {\n",
    "        \"Mean\": np.mean(LL),\n",
    "        \"StdDev\": np.std(LL),\n",
    "        \"Variance\": np.var(LL),\n",
    "        \"contrast\": np.mean(graycoprops(glcm, 'contrast')),\n",
    "        \"homogeneity\": np.mean(graycoprops(glcm, 'homogeneity')),\n",
    "        \"energy\": np.mean(graycoprops(glcm, 'energy')),\n",
    "        \"correlation\": np.mean(graycoprops(glcm, 'correlation')),\n",
    "        \"dissimilarity\": np.mean(graycoprops(glcm, 'dissimilarity')),\n",
    "        \"ASM\": np.mean(graycoprops(glcm, 'ASM')),  # Angular Second Moment\n",
    "        \"entropy\": shannon_entropy(LL)  # Entropy of the LL subband\n",
    "    }\n",
    "    return glcm_features\n",
    "\n",
    "# Initialize a list to store the extracted features\n",
    "data = []\n",
    "\n",
    "# Progress tracking\n",
    "total_images = sum(len(files) for _, _, files in os.walk(dataset_path) if files)\n",
    "processed_images = 0\n",
    "\n",
    "# Iterate through the dataset\n",
    "for category in categories:\n",
    "    category_path = os.path.join(dataset_path, category)\n",
    "    label = category  # Use the folder name as the label\n",
    "\n",
    "    for filename in os.listdir(category_path):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "            image_path = os.path.join(category_path, filename)\n",
    "            features = extract_dwt_glcm_features(image_path)\n",
    "            features[\"filename\"] = filename\n",
    "            features[\"category\"] = label\n",
    "            data.append(features)\n",
    "\n",
    "            # Update progress\n",
    "            processed_images += 1\n",
    "            progress = (processed_images / total_images) * 100\n",
    "            print(f\"Progress: {progress:.2f}% images processed\", end=\"\\r\")\n",
    "\n",
    "# Convert the extracted features into a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_csv_path = r\"D:\\new dataset\\new dataset\\dwt_glcm_features.csv\"\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"\\nFeature extraction completed. Data saved to {output_csv_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DWT+SWT+GLCM Features Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pywt\n",
    "from skimage import color, io\n",
    "from skimage.measure import shannon_entropy\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Path to your dataset folder (update this path)\n",
    "dataset_folder = r\"D:\\new dataset\\KTH_TIPS\"\n",
    "# Function to preprocess the image\n",
    "# Function to preprocess the image\n",
    "def preprocess_image(image_path, target_size=(128, 128)):\n",
    "    # Load the image\n",
    "    img = io.imread(image_path)\n",
    "    \n",
    "    # If the image has an alpha channel, remove it (convert RGBA to RGB)\n",
    "    if img.ndim == 3 and img.shape[-1] == 4:\n",
    "        img = img[:, :, :3]\n",
    "    \n",
    "    # Convert RGB to grayscale if the image is not already grayscale\n",
    "    if img.ndim == 3:  # Check if the image has 3 channels\n",
    "        grayscale_img = color.rgb2gray(img)\n",
    "    else:\n",
    "        grayscale_img = img  # The image is already grayscale\n",
    "    \n",
    "    # Resize the grayscale image to the target size\n",
    "    resized_img = cv2.resize(grayscale_img, target_size)\n",
    "    \n",
    "    # Normalize pixel values to the range [0, 1]\n",
    "    normalized_img = resized_img / 255.0\n",
    "    \n",
    "    return normalized_img\n",
    "\n",
    "# Function to calculate DWT/SWT features\n",
    "def calculate_wavelet_features(sub_band):\n",
    "    features = {\n",
    "        \"Mean\": np.mean(sub_band),\n",
    "        \"Std_Dev\": np.std(sub_band),\n",
    "        \"Variance\": np.var(sub_band),\n",
    "        \"Energy\": np.sum(np.square(sub_band)),\n",
    "        \"Entropy\": shannon_entropy(sub_band),\n",
    "        \"Contrast\": np.std(sub_band),\n",
    "        \"Homogeneity\": 1 / (1 + np.var(sub_band))\n",
    "    }\n",
    "    return features\n",
    "\n",
    "# Function to calculate correlation between wavelet sub-bands\n",
    "def calculate_correlation(sub_band1, sub_band2):\n",
    "    return np.corrcoef(sub_band1.flatten(), sub_band2.flatten())[0, 1]\n",
    "\n",
    "# Function to extract GLCM features\n",
    "def calculate_glcm_features(image, patch_size=7):\n",
    "    # Resize and preprocess for GLCM\n",
    "    resized_image = cv2.resize(image, (128, 128))\n",
    "    \n",
    "    # Scale the image to the range [0, 255] and convert to uint8\n",
    "    resized_image = (resized_image * 255).astype(np.uint8)\n",
    "    \n",
    "    # Initialize GLCM feature maps\n",
    "    contrast_map = np.zeros_like(resized_image, dtype=float)\n",
    "    homogeneity_map = np.zeros_like(resized_image, dtype=float)\n",
    "    energy_map = np.zeros_like(resized_image, dtype=float)\n",
    "    correlation_map = np.zeros_like(resized_image, dtype=float)\n",
    "    entropy_map = np.zeros_like(resized_image, dtype=float)\n",
    "\n",
    "    # Calculate local GLCM features using a sliding window\n",
    "    for i in range(0, resized_image.shape[0] - patch_size, patch_size):\n",
    "        for j in range(0, resized_image.shape[1] - patch_size, patch_size):\n",
    "            patch = resized_image[i:i + patch_size, j:j + patch_size]\n",
    "            patch_glcm = graycomatrix(patch, distances=[1], angles=[0], levels=256, symmetric=True, normed=True)\n",
    "            contrast_map[i:i + patch_size, j:j + patch_size] = graycoprops(patch_glcm, 'contrast')[0, 0]\n",
    "            homogeneity_map[i:i + patch_size, j:j + patch_size] = graycoprops(patch_glcm, 'homogeneity')[0, 0]\n",
    "            energy_map[i:i + patch_size, j:j + patch_size] = graycoprops(patch_glcm, 'energy')[0, 0]\n",
    "            correlation_map[i:i + patch_size, j:j + patch_size] = graycoprops(patch_glcm, 'correlation')[0, 0]\n",
    "            entropy_map[i:i + patch_size, j:j + patch_size] = -np.sum(patch_glcm * np.log2(patch_glcm + (patch_glcm == 0)))\n",
    "\n",
    "    # Compute global averages of the feature maps\n",
    "    features = {\n",
    "        \"GLCM_Contrast\": np.mean(contrast_map),\n",
    "        \"GLCM_Homogeneity\": np.mean(homogeneity_map),\n",
    "        \"GLCM_Energy\": np.mean(energy_map),\n",
    "        \"GLCM_Correlation\": np.mean(correlation_map),\n",
    "        \"GLCM_Entropy\": np.mean(entropy_map),\n",
    "        \"GLCM_Mean\": np.mean(resized_image),\n",
    "        \"GLCM_Std_Dev\": np.std(resized_image),\n",
    "        \"GLCM_Variance\": np.var(resized_image)\n",
    "    }\n",
    "    return features\n",
    "\n",
    "# Initialize a list to store feature data for the DataFrame\n",
    "feature_data = []\n",
    "\n",
    "# Loop through each class folder\n",
    "for class_name in os.listdir(dataset_folder):\n",
    "    class_path = os.path.join(dataset_folder, class_name)\n",
    "    \n",
    "    # Process only directories\n",
    "    if os.path.isdir(class_path):\n",
    "        total_files = len([f for f in os.listdir(class_path) if f.endswith(('.jpg', '.png', '.jpeg', '.jfif'))])\n",
    "        \n",
    "        with tqdm(total=total_files, desc=f\"Processing {class_name} images\", unit=\"file\") as pbar:\n",
    "            for file_name in os.listdir(class_path):\n",
    "                if file_name.endswith(('.jpg', '.png', '.jpeg', '.jfif')):\n",
    "                    image_path = os.path.join(class_path, file_name)\n",
    "                    \n",
    "                    # Preprocess the image\n",
    "                    preprocessed_image = preprocess_image(image_path)\n",
    "\n",
    "                    # Extract GLCM features\n",
    "                    glcm_features = calculate_glcm_features(preprocessed_image)\n",
    "\n",
    "                    # Perform DWT\n",
    "                    coeffs_dwt = pywt.dwt2(preprocessed_image, 'haar')\n",
    "                    LL_dwt, (LH_dwt, HL_dwt, HH_dwt) = coeffs_dwt\n",
    "                    \n",
    "                    # Extract DWT features\n",
    "                    dwt_features = {\n",
    "                        \"DWT_LL\": calculate_wavelet_features(LL_dwt),\n",
    "                        \"DWT_LH\": calculate_wavelet_features(LH_dwt),\n",
    "                        \"DWT_HL\": calculate_wavelet_features(HL_dwt),\n",
    "                        \"DWT_HH\": calculate_wavelet_features(HH_dwt)\n",
    "                    }\n",
    "\n",
    "                    # Perform SWT\n",
    "                    coeffs_swt = pywt.swt2(preprocessed_image, 'haar', level=1)\n",
    "                    LL_swt, (LH_swt, HL_swt, HH_swt) = coeffs_swt[0]\n",
    "\n",
    "                    # Extract SWT features\n",
    "                    swt_features = {\n",
    "                        \"SWT_LL\": calculate_wavelet_features(LL_swt),\n",
    "                        \"SWT_LH\": calculate_wavelet_features(LH_swt),\n",
    "                        \"SWT_HL\": calculate_wavelet_features(HL_swt),\n",
    "                        \"SWT_HH\": calculate_wavelet_features(HH_swt)\n",
    "                    }\n",
    "\n",
    "                    # Combine all features\n",
    "                    combined_features = {\n",
    "                        \"Class\": class_name,\n",
    "                        \"Image Name\": file_name,\n",
    "                        **glcm_features,\n",
    "                        **{f\"DWT_{key}_{k}\": v for key, subband in dwt_features.items() for k, v in subband.items()},\n",
    "                        **{f\"SWT_{key}_{k}\": v for key, subband in swt_features.items() for k, v in subband.items()}\n",
    "                    }\n",
    "\n",
    "                    # Append features\n",
    "                    feature_data.append(combined_features)\n",
    "                    pbar.update(1)\n",
    "\n",
    "# Create a DataFrame from the feature data\n",
    "df = pd.DataFrame(feature_data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_csv_path = r\"D:\\new dataset\\new dataset\\SWT+DWT+GLCMcombined_features.csv\"\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(\"Feature extraction completed. Data saved to:\", output_csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CNN Features Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Step 1: Load the Pre-trained CNN Model\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Step 2: Define Dataset Directory and Output File\n",
    "dataset_dir =r\"D:\\new dataset\\KTH_TIPS\" # Replace with your dataset path\n",
    "output_csv_path = r\"D:\\new dataset\\new dataset\\cnn\\CNN_features.csv\"  # Replace with desired CSV output path\n",
    "\n",
    "# Step 3: Initialize Lists to Store Features, Labels, and Filenames\n",
    "features_list = []\n",
    "labels_list = []\n",
    "image_filenames = []\n",
    "\n",
    "# Step 4: Process Each Image in the Subfolders\n",
    "for subdir, _, files in os.walk(dataset_dir):\n",
    "    label = os.path.basename(subdir)  # Use subfolder name as the label\n",
    "    if label == os.path.basename(dataset_dir):\n",
    "        continue  # Skip the root folder\n",
    "\n",
    "    print(f\"Processing images in folder: {label}\")\n",
    "    for file in tqdm(files, desc=f\"Processing {label}\"):\n",
    "        file_path = os.path.join(subdir, file)\n",
    "        \n",
    "        try:\n",
    "            # Load and preprocess the image\n",
    "            image = load_img(file_path, target_size=(224, 224))  # Resize to match VGG16 input\n",
    "            image_array = img_to_array(image)\n",
    "            image_array = np.expand_dims(image_array, axis=0)  # Add batch dimension\n",
    "            image_array = preprocess_input(image_array)  # Preprocess as per VGG16 requirements\n",
    "\n",
    "            # Extract features using the CNN model\n",
    "            features = base_model.predict(image_array)\n",
    "            flattened_features = features.flatten()  # Flatten the features into a 1D array\n",
    "\n",
    "            # Append data to lists\n",
    "            features_list.append(flattened_features)\n",
    "            labels_list.append(label)\n",
    "            image_filenames.append(file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "# Step 5: Perform Feature Selection (Filter out low-variance features)\n",
    "print(\"Performing feature selection to retain important features...\")\n",
    "features_array = np.array(features_list)\n",
    "selector = VarianceThreshold(threshold=0.01)  # Set a variance threshold (adjustable)\n",
    "selected_features = selector.fit_transform(features_array)\n",
    "\n",
    "# Step 6: Save the Filtered Features to a CSV File\n",
    "print(\"Saving filtered features to CSV...\")\n",
    "filtered_features_df = pd.DataFrame(selected_features)\n",
    "filtered_features_df['category'] = labels_list  # Add labels as a column\n",
    "filtered_features_df['filename'] = image_filenames  # Add filenames as a column\n",
    "\n",
    "filtered_features_df.to_csv(output_csv_path, index=False)\n",
    "print(f\"Filtered features saved successfully to: {output_csv_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
