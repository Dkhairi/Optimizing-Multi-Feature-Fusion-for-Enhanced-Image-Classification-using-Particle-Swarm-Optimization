{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**models training on DWT features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Step 1: Load extracted features\n",
    "input_csv_path = r\"C:\\Users\\123\\OneDrive\\Desktop\\improved_wavelet_features.csv\"\n",
    "df = pd.read_csv(input_csv_path)\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.drop(columns=[\"category\", \"filename\"])  # Drop non-numeric columns\n",
    "y = df[\"category\"]\n",
    "\n",
    "# Step 2: Preprocess the data\n",
    "# Normalize the features\n",
    "scaler = MinMaxScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_normalized, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "# Step 3: Train Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Evaluate the model\n",
    "y_pred_train = rf_model.predict(X_train)\n",
    "y_pred_test = rf_model.predict(X_test)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "# Print accuracy scores\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report (Test Data):\\n\", classification_report(y_test, y_pred_test, target_names=label_encoder.classes_))\n",
    "\n",
    "# Step 5: Plot Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Step 6: Plot Training vs Testing Accuracy\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar([\"Training Accuracy\", \"Testing Accuracy\"], [train_accuracy, test_accuracy], color=['blue', 'orange'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "plt.title('Training vs Testing Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "# Load the extracted features\n",
    "features_csv_path = r\"C:\\Users\\123\\OneDrive\\Desktop\\improved_wavelet_features.csv\"\n",
    "data = pd.read_csv(features_csv_path)\n",
    "\n",
    "# Separate features (X) and labels (y)\n",
    "X = data.drop(columns=[\"category\", \"filename\"])  # Drop non-feature columns\n",
    "y = data[\"category\"]\n",
    "\n",
    "# Split the data into training and test sets (stratify ensures balanced splits)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=120, stratify=y)\n",
    "\n",
    "# Standardize features (important for SVM performance)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the SVM parameter grid for optimization\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],         # Regularization parameter\n",
    "    'kernel': ['linear', 'rbf'],    # Linear and RBF kernels\n",
    "    'gamma': ['scale', 'auto'],     # Kernel coefficient\n",
    "}\n",
    "\n",
    "# Initialize the SVM classifier and GridSearchCV\n",
    "svm = SVC(probability=True)\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=svm,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,                           # 3-fold cross-validation for efficiency\n",
    "    scoring='accuracy',             # Use accuracy as the scoring metric\n",
    "    verbose=2,\n",
    "    n_jobs=-1                       # Use all available CPU cores\n",
    ")\n",
    "\n",
    "# Train the SVM model with grid search\n",
    "print(\"Training the SVM model...\")\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best model from grid search\n",
    "best_svm = grid_search.best_estimator_\n",
    "print(\"\\nBest SVM Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "print(\"\\nEvaluating the model...\")\n",
    "y_pred_train = best_svm.predict(X_train_scaled)\n",
    "y_pred_test = best_svm.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"\\nTraining Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report (Test Data):\\n\", classification_report(y_test, y_pred_test))\n",
    "\n",
    "# Save the trained SVM model and scaler for future use\n",
    "joblib.dump(best_svm, r\"C:\\Users\\123\\OneDrive\\Desktop\\svm_model.pkl\")\n",
    "joblib.dump(scaler, r\"C:\\Users\\123\\OneDrive\\Desktop\\scaler.pkl\")\n",
    "\n",
    "print(\"\\nModel training and saving completed.\")\n",
    "\n",
    "# Step 5: Plot Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=y.unique(), yticklabels=y.unique())\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Step 6: Plot Training vs Testing Accuracy\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar([\"Training Accuracy\", \"Testing Accuracy\"], [train_accuracy, test_accuracy], color=['blue', 'orange'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "plt.title('Training vs Testing Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Path to the feature CSV file\n",
    "csv_path =  r\"C:\\Users\\123\\OneDrive\\Desktop\\improved_wavelet_features.csv\"\n",
    "\n",
    "# Load the extracted features dataset\n",
    "data = pd.read_csv(csv_path)\n",
    "\n",
    "# Encode the class labels into numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "data['category'] = label_encoder.fit_transform(data['category'])\n",
    "\n",
    "# Separate features (X) and target labels (y)\n",
    "X = data.drop(columns=['category', 'filename'])  # Drop target and metadata columns\n",
    "y = data['category']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# One-hot encode the labels for CNN\n",
    "num_classes = len(np.unique(y))\n",
    "y_categorical = to_categorical(y, num_classes)\n",
    "\n",
    "# Reshape the features for CNN input (add a channel dimension)\n",
    "X_scaled = X_scaled[..., np.newaxis]\n",
    "\n",
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_categorical, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Define the shallow CNN model\n",
    "model = Sequential([\n",
    "    Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.2),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=150, batch_size=32, validation_split=0.2, verbose=2)\n",
    "\n",
    "# Evaluate the model\n",
    "train_loss, train_accuracy = model.evaluate(X_train, y_train, verbose=0)\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print training and testing accuracy\n",
    "print(f\"\\nTraining Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_test_pred_classes = np.argmax(y_test_pred, axis=1)\n",
    "y_test_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_true_classes, y_test_pred_classes, target_names=label_encoder.classes_))\n",
    "\n",
    "# Compute and plot confusion matrix\n",
    "cm = confusion_matrix(y_test_true_classes, y_test_pred_classes)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Models training on SWT features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "\n",
    "# Path to the feature CSV file\n",
    "csv_path =  r\"C:\\Users\\123\\OneDrive\\Desktop\\features extraction\\SWT\\SWT_features_with_labels.csv\"\n",
    "\n",
    "# Load the extracted features dataset\n",
    "data = pd.read_csv(csv_path)\n",
    "\n",
    "# Encode the class labels into numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "data['category'] = label_encoder.fit_transform(data['category'])\n",
    "\n",
    "# Separate features and target labels\n",
    "X = data.drop(columns=['category', 'filename'])  # Drop target and metadata\n",
    "y = data['category']\n",
    "\n",
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict the classes for the training set (for training accuracy)\n",
    "y_train_pred = rf_classifier.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "# Predict the classes for the test set (for testing accuracy)\n",
    "y_test_pred = rf_classifier.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Print the training and testing accuracy\n",
    "print(f\"Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"Testing Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Evaluate the classifier's performance on the test set\n",
    "print(\"\\nClassification Report (Test Set):\\n\", classification_report(y_test, y_test_pred))\n",
    "\n",
    "# Plot confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the extracted features\n",
    "input_csv_path = r\"C:\\Users\\123\\OneDrive\\Desktop\\features extraction\\SWT\\SWT_features_with_labels.csv\"\n",
    "df = pd.read_csv(input_csv_path)\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.drop(columns=[\"category\", \"filename\"])  # Drop non-feature columns\n",
    "y = df[\"category\"]  # Labels\n",
    "\n",
    "# Encode the class labels into numeric form\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Perform feature selection (e.g., top 50 features based on ANOVA F-test)\n",
    "selector = SelectKBest(score_func=f_classif, k=50)\n",
    "X_selected = selector.fit_transform(X, y_encoded)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the SVM model\n",
    "svm = SVC(random_state=42, probability=True)\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.1, 0.01, 0.001],\n",
    "    'kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(svm, param_grid, scoring='accuracy', cv=5, verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best model from GridSearchCV\n",
    "best_svm = grid_search.best_estimator_\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Evaluate the model on the training set (for training accuracy)\n",
    "y_train_pred = best_svm.predict(X_train_scaled)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(f\"\\nTraining Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Evaluate the model on the test set (for testing accuracy)\n",
    "y_test_pred = best_svm.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Generate and display the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Plot the confusion matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.show()\n",
    "\n",
    "# Print the classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Path to the feature CSV file\n",
    "csv_path = r\"C:\\Users\\123\\OneDrive\\Desktop\\features extraction\\SWT\\SWT_features_with_labels.csv\"\n",
    "\n",
    "# Load the extracted features dataset\n",
    "data = pd.read_csv(csv_path)\n",
    "\n",
    "# Encode the class labels into numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "data['category'] = label_encoder.fit_transform(data['category'])\n",
    "\n",
    "# Separate features (X) and target labels (y)\n",
    "X = data.drop(columns=['category', 'filename'])  # Drop target and metadata columns\n",
    "y = data['category']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# One-hot encode the labels for CNN\n",
    "num_classes = len(np.unique(y))\n",
    "y_categorical = to_categorical(y, num_classes)\n",
    "\n",
    "# Reshape the features for CNN input (add a channel dimension)\n",
    "X_scaled = X_scaled[..., np.newaxis]\n",
    "\n",
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_categorical, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Define the shallow CNN model\n",
    "model = Sequential([\n",
    "    Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.2),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=150, batch_size=32, validation_split=0.2, verbose=2)\n",
    "\n",
    "# Evaluate the model\n",
    "train_loss, train_accuracy = model.evaluate(X_train, y_train, verbose=0)\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print training and testing accuracy\n",
    "print(f\"\\nTraining Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_test_pred_classes = np.argmax(y_test_pred, axis=1)\n",
    "y_test_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_true_classes, y_test_pred_classes, target_names=label_encoder.classes_))\n",
    "\n",
    "# Compute and plot confusion matrix\n",
    "cm = confusion_matrix(y_test_true_classes, y_test_pred_classes)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Models Training on glcm features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "# Load the extracted features\n",
    "features_csv_path = r\"C:\\Users\\123\\OneDrive\\Desktop\\custom\\glcm_features.csv\"\n",
    "data = pd.read_csv(features_csv_path)\n",
    "\n",
    "# Separate features (X) and labels (y)\n",
    "X = data.drop(columns=[\"filename\", \"category\"])  # Drop non-feature columns\n",
    "y = data[\"category\"]\n",
    "\n",
    "# Split the data into training and test sets (stratify ensures balanced splits)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=120, stratify=y)\n",
    "\n",
    "# Standardize features (important for SVM performance)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the SVM parameter grid for optimization\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],         # Regularization parameter\n",
    "    'kernel': ['linear', 'rbf'],    # Linear and RBF kernels\n",
    "    'gamma': ['scale', 'auto'],     # Kernel coefficient\n",
    "}\n",
    "\n",
    "# Initialize the SVM classifier and GridSearchCV\n",
    "svm = SVC(probability=True)\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=svm,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,                           # 3-fold cross-validation\n",
    "    scoring='accuracy',             \n",
    "    verbose=2,\n",
    "    n_jobs=-1                       \n",
    ")\n",
    "\n",
    "# Train the SVM model with grid search\n",
    "print(\"Training the SVM model...\")\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best model from grid search\n",
    "best_svm = grid_search.best_estimator_\n",
    "print(\"\\nBest SVM Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "print(\"\\nEvaluating the model...\")\n",
    "y_pred_train = best_svm.predict(X_train_scaled)\n",
    "y_pred_test = best_svm.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"\\nTraining Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Print classification report\n",
    "class_report = classification_report(y_test, y_pred_test, output_dict=True)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_test))\n",
    "\n",
    "# Save the trained SVM model and scaler for future use\n",
    "joblib.dump(best_svm, r\"C:\\Users\\123\\OneDrive\\Desktop\\glcm_svm_model.pkl\")\n",
    "joblib.dump(scaler, r\"C:\\Users\\123\\OneDrive\\Desktop\\glcm_scaler.pkl\")\n",
    "\n",
    "print(\"\\nModel training and saving completed.\")\n",
    "\n",
    "# Step 5: Plot Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=y.unique(), yticklabels=y.unique())\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Step 6: Plot Training vs Testing Accuracy\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar([\"Training Accuracy\", \"Testing Accuracy\"], [train_accuracy, test_accuracy], color=['blue', 'orange'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "plt.title('Training vs Testing Accuracy')\n",
    "plt.show()\n",
    "\n",
    "# Step 7: Plot Precision, Recall, and F1-score per class\n",
    "report_df = pd.DataFrame(class_report).transpose().iloc[:-1, :]  # Exclude 'accuracy' row\n",
    "report_df[['precision', 'recall', 'f1-score']].plot(kind='bar', figsize=(10, 6))\n",
    "plt.title(\"Precision, Recall & F1-score per Class\")\n",
    "plt.xlabel(\"Classes\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Step 1: Load extracted features\n",
    "input_csv_path =  r\"C:\\Users\\123\\OneDrive\\Desktop\\custom\\glcm_features.csv\"\n",
    "df = pd.read_csv(input_csv_path)\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.drop(columns=[\"category\", \"filename\"])  # Drop non-numeric columns\n",
    "y = df[\"category\"]\n",
    "\n",
    "# Step 2: Preprocess the data\n",
    "# Normalize the features\n",
    "scaler = MinMaxScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_normalized, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "# Step 3: Train Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Evaluate the model\n",
    "y_pred_train = rf_model.predict(X_train)\n",
    "y_pred_test = rf_model.predict(X_test)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "# Print accuracy scores\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report (Test Data):\\n\", classification_report(y_test, y_pred_test, target_names=label_encoder.classes_))\n",
    "\n",
    "# Step 5: Plot Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Step 6: Plot Training vs Testing Accuracy\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar([\"Training Accuracy\", \"Testing Accuracy\"], [train_accuracy, test_accuracy], color=['blue', 'orange'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "plt.title('Training vs Testing Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Path to the feature CSV file\n",
    "csv_path =  r\"C:\\Users\\123\\OneDrive\\Desktop\\custom\\glcm_features.csv\"\n",
    "\n",
    "# Load the extracted features dataset\n",
    "data = pd.read_csv(csv_path)\n",
    "\n",
    "# Encode the class labels into numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "data['category'] = label_encoder.fit_transform(data['category'])\n",
    "\n",
    "# Separate features (X) and target labels (y)\n",
    "X = data.drop(columns=['category', 'filename'])  # Drop target and metadata columns\n",
    "y = data['category']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# One-hot encode the labels for CNN\n",
    "num_classes = len(np.unique(y))\n",
    "y_categorical = to_categorical(y, num_classes)\n",
    "\n",
    "# Reshape the features for CNN input (add a channel dimension)\n",
    "X_scaled = X_scaled[..., np.newaxis]\n",
    "\n",
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_categorical, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Define the shallow CNN model\n",
    "model = Sequential([\n",
    "    Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.2),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=150, batch_size=32, validation_split=0.2, verbose=2)\n",
    "\n",
    "# Evaluate the model\n",
    "train_loss, train_accuracy = model.evaluate(X_train, y_train, verbose=0)\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print training and testing accuracy\n",
    "print(f\"\\nTraining Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_test_pred_classes = np.argmax(y_test_pred, axis=1)\n",
    "y_test_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_true_classes, y_test_pred_classes, target_names=label_encoder.classes_))\n",
    "\n",
    "# Compute and plot confusion matrix\n",
    "cm = confusion_matrix(y_test_true_classes, y_test_pred_classes)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Models Trainig on SWT+GLCM Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Step 1: Load extracted features\n",
    "input_csv_path =  r\"C:\\Users\\123\\OneDrive\\Desktop\\custom\\swt_glcm_features.csv\"\n",
    "df = pd.read_csv(input_csv_path)\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.drop(columns=[\"category\", \"filename\"])  # Drop non-numeric columns\n",
    "y = df[\"category\"]\n",
    "\n",
    "# Step 2: Preprocess the data\n",
    "# Normalize the features\n",
    "scaler = MinMaxScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_normalized, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "# Step 3: Train Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Evaluate the model\n",
    "y_pred_train = rf_model.predict(X_train)\n",
    "y_pred_test = rf_model.predict(X_test)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "# Print accuracy scores\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report (Test Data):\\n\", classification_report(y_test, y_pred_test, target_names=label_encoder.classes_))\n",
    "\n",
    "# Step 5: Plot Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Step 6: Plot Training vs Testing Accuracy\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar([\"Training Accuracy\", \"Testing Accuracy\"], [train_accuracy, test_accuracy], color=['blue', 'orange'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "plt.title('Training vs Testing Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "# Load the dataset\n",
    "input_csv_path = r\"C:\\Users\\123\\OneDrive\\Desktop\\custom\\swt_glcm_features.csv\"  # Path to the feature file\n",
    "df = pd.read_csv(input_csv_path)\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.drop(columns=[\"filename\", \"category\"])  # Drop non-feature columns\n",
    "y = df[\"category\"]  # Labels\n",
    "\n",
    "# Encode the labels into numeric form\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=100, stratify=y_encoded)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the SVM model\n",
    "svm = SVC(probability=True, random_state=100)\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best parameters and the corresponding SVM model\n",
    "best_params = grid_search.best_params_\n",
    "best_svm = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_train_pred = best_svm.predict(X_train_scaled)\n",
    "y_test_pred = best_svm.predict(X_test_scaled)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"\\nTraining Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# Plot confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Save the trained model and the scaler\n",
    "joblib.dump(best_svm, r\"C:\\Users\\123\\OneDrive\\Desktop\\custom\\(svm+glcm)svm_model.pkl\")\n",
    "joblib.dump(scaler, r\"C:\\Users\\123\\OneDrive\\Desktop\\custom\\(svm+glcm)scaler.pkl\")\n",
    "joblib.dump(label_encoder, \"label_encoder.pkl\")\n",
    "\n",
    "print(\"\\nSVM model, scaler, and label encoder saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Path to the feature CSV file\n",
    "csv_path = r\"C:\\Users\\123\\OneDrive\\Desktop\\custom\\swt_glcm_features.csv\"\n",
    "\n",
    "# Load the extracted features dataset\n",
    "data = pd.read_csv(csv_path)\n",
    "\n",
    "# Encode the class labels into numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "data['category'] = label_encoder.fit_transform(data['category'])\n",
    "\n",
    "# Separate features (X) and target labels (y)\n",
    "X = data.drop(columns=['category', 'filename'])  # Drop target and metadata columns\n",
    "y = data['category']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# One-hot encode the labels for CNN\n",
    "num_classes = len(np.unique(y))\n",
    "y_categorical = to_categorical(y, num_classes)\n",
    "\n",
    "# Reshape the features for CNN input (add a channel dimension)\n",
    "X_scaled = X_scaled[..., np.newaxis]\n",
    "\n",
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_categorical, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Define the shallow CNN model\n",
    "model = Sequential([\n",
    "    Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.2),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=150, batch_size=32, validation_split=0.2, verbose=2)\n",
    "\n",
    "# Evaluate the model\n",
    "train_loss, train_accuracy = model.evaluate(X_train, y_train, verbose=0)\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print training and testing accuracy\n",
    "print(f\"\\nTraining Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_test_pred_classes = np.argmax(y_test_pred, axis=1)\n",
    "y_test_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_true_classes, y_test_pred_classes, target_names=label_encoder.classes_))\n",
    "\n",
    "# Compute and plot confusion matrix\n",
    "cm = confusion_matrix(y_test_true_classes, y_test_pred_classes)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model training on dwt+glcm features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "# Load the dataset\n",
    "input_csv_path = r\"C:\\Users\\123\\OneDrive\\Desktop\\custom\\dwt_glcm_features.csv\"  # Path to the feature file\n",
    "df = pd.read_csv(input_csv_path)\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.drop(columns=[\"filename\", \"category\"])  # Drop non-feature columns\n",
    "y = df[\"category\"]  # Labels\n",
    "\n",
    "# Encode the labels into numeric form\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=100, stratify=y_encoded)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the SVM model\n",
    "svm = SVC(probability=True, random_state=100)\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best parameters and the corresponding SVM model\n",
    "best_params = grid_search.best_params_\n",
    "best_svm = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_train_pred = best_svm.predict(X_train_scaled)\n",
    "y_test_pred = best_svm.predict(X_test_scaled)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"\\nTraining Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# Plot confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Save the trained model and the scaler\n",
    "joblib.dump(best_svm, r\"C:\\Users\\123\\OneDrive\\Desktop\\custom\\dwt+glcm_svm_model.pkl\")\n",
    "joblib.dump(scaler, r\"C:\\Users\\123\\OneDrive\\Desktop\\custom\\dwt+glcm_scaler.pkl\")\n",
    "joblib.dump(label_encoder, \"label_encoder_dwt+glcm.pkl\")\n",
    "\n",
    "print(\"\\nSVM model, scaler, and label encoder saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Step 1: Load extracted features\n",
    "input_csv_path =  r\"C:\\Users\\123\\OneDrive\\Desktop\\custom\\dwt_glcm_features.csv\"\n",
    "df = pd.read_csv(input_csv_path)\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.drop(columns=[\"category\", \"filename\"])  # Drop non-numeric columns\n",
    "y = df[\"category\"]\n",
    "\n",
    "# Step 2: Preprocess the data\n",
    "# Normalize the features\n",
    "scaler = MinMaxScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_normalized, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "# Step 3: Train Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Evaluate the model\n",
    "y_pred_train = rf_model.predict(X_train)\n",
    "y_pred_test = rf_model.predict(X_test)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "# Print accuracy scores\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report (Test Data):\\n\", classification_report(y_test, y_pred_test, target_names=label_encoder.classes_))\n",
    "\n",
    "# Step 5: Plot Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Step 6: Plot Training vs Testing Accuracy\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar([\"Training Accuracy\", \"Testing Accuracy\"], [train_accuracy, test_accuracy], color=['blue', 'orange'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "plt.title('Training vs Testing Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Path to the feature CSV file\n",
    "csv_path = r\"C:\\Users\\123\\OneDrive\\Desktop\\custom\\dwt_glcm_features.csv\"\n",
    "\n",
    "# Load the extracted features dataset\n",
    "data = pd.read_csv(csv_path)\n",
    "\n",
    "# Encode the class labels into numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "data['category'] = label_encoder.fit_transform(data['category'])\n",
    "\n",
    "# Separate features (X) and target labels (y)\n",
    "X = data.drop(columns=['category', 'filename'])  # Drop target and metadata columns\n",
    "y = data['category']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# One-hot encode the labels for CNN\n",
    "num_classes = len(np.unique(y))\n",
    "y_categorical = to_categorical(y, num_classes)\n",
    "\n",
    "# Reshape the features for CNN input (add a channel dimension)\n",
    "X_scaled = X_scaled[..., np.newaxis]\n",
    "\n",
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_categorical, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Define the shallow CNN model\n",
    "model = Sequential([\n",
    "    Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.2),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=150, batch_size=32, validation_split=0.2, verbose=2)\n",
    "\n",
    "# Evaluate the model\n",
    "train_loss, train_accuracy = model.evaluate(X_train, y_train, verbose=0)\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print training and testing accuracy\n",
    "print(f\"\\nTraining Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_test_pred_classes = np.argmax(y_test_pred, axis=1)\n",
    "y_test_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_true_classes, y_test_pred_classes, target_names=label_encoder.classes_))\n",
    "\n",
    "# Compute and plot confusion matrix\n",
    "cm = confusion_matrix(y_test_true_classes, y_test_pred_classes)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Models Training on CNN features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "# Step 1: Load the Extracted Features and Labels\n",
    "csv_file_path = r\"C:\\Users\\123\\OneDrive\\Desktop\\features extraction\\cnn\\CNN_fruit_features_filtered.csv\" # Update your file path\n",
    "data = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Separate features and labels\n",
    "X = data.drop(columns=['Label', 'Filename'])  # Drop class and image name columns\n",
    "y = data['Label']  # Target labels (class)\n",
    "\n",
    "# Step 2: Split Data into Training and Testing Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Step 3: Initialize and Train Random Forest Model\n",
    "print(\"Training Random Forest classifier...\")\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,  # Number of trees\n",
    "    random_state=42,\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    class_weight=\"balanced\"  # Handle class imbalances if any\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Evaluate the Model on the Training Set\n",
    "y_train_pred = rf_model.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(f\"Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Step 5: Evaluate the Model on the Test Set\n",
    "y_test_pred = rf_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "# Step 6: Compute and plot confusion matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "labels = sorted(y.unique())  # Get class labels from the dataset\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Path to the feature CSV file\n",
    "csv_path = r\"C:\\Users\\123\\OneDrive\\Desktop\\features extraction\\cnn\\CNN_fruit_features_filtered.csv\"\n",
    "\n",
    "# Load the extracted features dataset\n",
    "data = pd.read_csv(csv_path)\n",
    "\n",
    "# Encode the class labels into numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "data['Label'] = label_encoder.fit_transform(data['Label'])\n",
    "\n",
    "# Separate features (X) and target labels (y)\n",
    "X = data.drop(columns=['Label', 'Filename'])  # Drop target and metadata columns\n",
    "y = data['Label']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# One-hot encode the labels for CNN\n",
    "num_classes = len(np.unique(y))\n",
    "y_categorical = to_categorical(y, num_classes)\n",
    "\n",
    "# Reshape the features for CNN input (add a channel dimension)\n",
    "X_scaled = X_scaled[..., np.newaxis]\n",
    "\n",
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_categorical, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Define the shallow CNN model\n",
    "model = Sequential([\n",
    "    Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.2),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=250, batch_size=32, validation_split=0.2, verbose=2)\n",
    "\n",
    "# Evaluate the model\n",
    "train_loss, train_accuracy = model.evaluate(X_train, y_train, verbose=0)\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print training and testing accuracy\n",
    "print(f\"\\nTraining Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_test_pred_classes = np.argmax(y_test_pred, axis=1)\n",
    "y_test_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_true_classes, y_test_pred_classes, target_names=label_encoder.classes_))\n",
    "\n",
    "# Compute and plot confusion matrix\n",
    "cm = confusion_matrix(y_test_true_classes, y_test_pred_classes)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Save the model\n",
    "model_save_path = r\"C:\\Users\\123\\OneDrive\\Desktop\\features extraction\\cnn\\shallow_cnn_model.h5\"\n",
    "model.save(model_save_path)\n",
    "print(\"\\nModel saved to:\", model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "# Load the extracted features\n",
    "input_csv_path = r\"C:\\Users\\123\\OneDrive\\Desktop\\features extraction\\cnn\\CNN_fruit_features_filtered.csv\"\n",
    "df = pd.read_csv(input_csv_path)\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.drop(columns=[\"Label\", \"Filename\"])  # Drop non-feature columns\n",
    "y = df[\"Label\"]  # Labels\n",
    "\n",
    "# Encode the class labels into numeric form\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# 🔹 **Fix 1: Apply PCA for Dimensionality Reduction (Alternative to SelectKBest)**\n",
    "pca = PCA(n_components=200)  # Reduce to 200 principal components\n",
    "X_reduced = pca.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the SVM model\n",
    "svm = SVC(random_state=42, probability=True)\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.1, 0.01, 0.001],\n",
    "    'kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(svm, param_grid, scoring='accuracy', cv=5, verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best model from GridSearchCV\n",
    "best_svm = grid_search.best_estimator_\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Evaluate the model on the training set\n",
    "y_train_pred = best_svm.predict(X_train_scaled)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(f\"\\nTraining Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_test_pred = best_svm.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Generate and display the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Plot the confusion matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.show()\n",
    "\n",
    "# Print the classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=label_encoder.classes_))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
